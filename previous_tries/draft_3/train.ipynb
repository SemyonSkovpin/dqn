{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cd5de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n"
     ]
    }
   ],
   "source": [
    "# For interactive plotting \n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c76fd1-324d-4ab6-b819-c58045323902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c76818d8-e24d-470f-ac67-b56e79d9239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "env_name = 'LunarLander-v3'\n",
    "seed = 42\n",
    "main_net_path = 'main_net.pth'\n",
    "\n",
    "mini_batch_size = 128\n",
    "buffer_size_limit = 10000\n",
    "steps_until_value_iteration = 500\n",
    "steps_until_target_net_update = 2000\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.1\n",
    "epsilon_decay = 0.999\n",
    "epsilon = epsilon_start\n",
    "lr=1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c8e6f7-1416-4c88-9659-268fb62bc2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7be5b2340e90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Environment and seeds\n",
    "env = gym.make(env_name, render_mode='rgb_array')\n",
    "\n",
    "env.reset(seed=seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e353e0-26fd-40c0-89b2-6ad6099f390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main q-network and target q-network\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "def make_mlp():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(observation_size, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, action_size)\n",
    "    )\n",
    "\n",
    "if os.path.exists(main_net_path):\n",
    "    main_net = torch.load(main_net_path)\n",
    "else:\n",
    "    main_net = make_mlp()\n",
    "    torch.save(main_net, main_net_path)\n",
    "target_net = make_mlp()\n",
    "\n",
    "def update_target_net():\n",
    "    target_net.load_state_dict(main_net.state_dict())\n",
    "\n",
    "update_target_net() # They are same from the start\n",
    "\n",
    "optimiser = optim.Adam(main_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb60f4b7-d6ca-4f3d-acd9-2f7c45452351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement expsilon-greedy policy\n",
    "def get_action(observation):\n",
    "    '''\n",
    "    observation: numpy array returned by env.step()\n",
    "    returns: integer action\n",
    "    '''\n",
    "    possible_actions = [i for i in range(env.action_space.n)]\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(possible_actions)\n",
    "\n",
    "    observation = torch.as_tensor(observation, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        q_star_per_each_action = main_net(observation)\n",
    "        action = torch.argmax(q_star_per_each_action).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11feab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon decay \n",
    "def decrease_epsilon():\n",
    "    global epsilon\n",
    "    epsilon = max(epsilon_end, epsilon * epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49276b42-32af-4e4f-84a7-afd4d55455e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = deque([], maxlen=buffer_size_limit)\n",
    "\n",
    "Timestep   = namedtuple('timestep',   [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]) \n",
    "'''\n",
    "Data types of Timestep:\n",
    "state/next_state is numpy array; \n",
    "action is int; \n",
    "reward is float; \n",
    "done is bool;\n",
    "'''\n",
    "\n",
    "Mini_batch = namedtuple('mini_batch', [\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"])\n",
    "\n",
    "\n",
    "def record_timestep(timestep):\n",
    "    replay_buffer.append(timestep)\n",
    "\n",
    "\n",
    "def sample_a_mini_batch():\n",
    "    '''\n",
    "    returns: named tuple with 5 1d tensors\n",
    "    '''\n",
    "    mini_batch = random.sample(replay_buffer, mini_batch_size)\n",
    "    mini_batch = list(zip(*mini_batch)) # Transpose\n",
    "\n",
    "    # Convert list of ndarrays to ndarray because Creating a tensor from a list of numpy.ndarrays is extremely slow. \n",
    "    states = np.array(mini_batch[0])\n",
    "    next_states = np.array(mini_batch[3])\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(mini_batch[1], dtype=torch.int64)\n",
    "    rewards = torch.tensor(mini_batch[2], dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(mini_batch[4], dtype = torch.bool)\n",
    "    \n",
    "    return Mini_batch(states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94af1fd9-9c67-43af-8712-be28d6b20a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mini_batch to get loss\n",
    "def compute_loss(mini_batch):\n",
    "    # Compute targets \n",
    "    v_star_of_next_states = torch.max(target_net(mini_batch.next_states), dim=1)[0]\n",
    "    v_star_of_next_states = v_star_of_next_states * (~mini_batch.dones) \n",
    "    y = mini_batch.rewards + gamma * v_star_of_next_states \n",
    "\n",
    "    # Compute main_net's predictions\n",
    "    predictions = main_net(mini_batch.states).gather(1, mini_batch.actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Loss \n",
    "    loss = nn.functional.mse_loss(predictions, y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c6fe3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)\n",
      "No XVisualInfo for format QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)\n",
      "Falling back to using screens root_visual.\n"
     ]
    }
   ],
   "source": [
    "# Plot episode returns throughout training\n",
    "episode_returns = []\n",
    "smoothed_returns = []\n",
    "smooth_alpha = 0.01  # Lower = smoother\n",
    "current_episode_rewards = []\n",
    "\n",
    "# Create a separate window for the plot at the start\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "fig.canvas.manager.set_window_title('DQN Training Progress')\n",
    "returns_line, = ax.plot([], [], label='Episode Return', alpha=0.5)\n",
    "smooth_line, = ax.plot([], [], label='Smoothed Return', color='orange')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Return')\n",
    "ax.legend()\n",
    "fig.show()\n",
    "\n",
    "def log_reward_for_plotting(reward):\n",
    "    global current_episode_rewards\n",
    "    current_episode_rewards.append(reward)\n",
    "\n",
    "def update_plot():\n",
    "    global episode_returns, smoothed_returns, current_episode_rewards\n",
    "    episode_return = sum(current_episode_rewards)\n",
    "    episode_returns.append(episode_return)\n",
    "    # Exponential moving average for smoothing\n",
    "    if smoothed_returns:\n",
    "        new_smooth = smooth_alpha * episode_return + (1 - smooth_alpha) * smoothed_returns[-1]\n",
    "    else:\n",
    "        new_smooth = episode_return\n",
    "    smoothed_returns.append(new_smooth)\n",
    "    current_episode_rewards = []\n",
    "\n",
    "    returns_line.set_data(range(len(episode_returns)), episode_returns)\n",
    "    smooth_line.set_data(range(len(smoothed_returns)), smoothed_returns)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94f8e8e-90dc-4c12-953f-d4a41e0e8ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)\n",
      "No XVisualInfo for format QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)\n",
      "Falling back to using screens root_visual.\n",
      "qt.glx: qglx_findConfig: Failed to finding matching FBConfig for QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)\n",
      "No XVisualInfo for format QSurfaceFormat(version 2.0, options QFlags<QSurfaceFormat::FormatOption>(), depthBufferSize -1, redBufferSize 1, greenBufferSize 1, blueBufferSize 1, alphaBufferSize -1, stencilBufferSize -1, samples -1, swapBehavior QSurfaceFormat::SingleBuffer, swapInterval 1, colorSpace QSurfaceFormat::DefaultColorSpace, profile  QSurfaceFormat::NoProfile)\n",
      "Falling back to using screens root_visual.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      8\u001b[0m action \u001b[38;5;241m=\u001b[39m get_action(observation)\n\u001b[0;32m---> 10\u001b[0m new_observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     11\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated \n\u001b[1;32m     13\u001b[0m timestep \u001b[38;5;241m=\u001b[39m Timestep(observation, action, reward, new_observation, done)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:621\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    607\u001b[0m         p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    608\u001b[0m             (\n\u001b[1;32m    609\u001b[0m                 ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    616\u001b[0m         (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    617\u001b[0m         impulse_pos,\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    619\u001b[0m     )\n\u001b[0;32m--> 621\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld\u001b[38;5;241m.\u001b[39mStep(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m FPS, \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[1;32m    623\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition\n\u001b[1;32m    624\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/gymnasium/envs/box2d/lunar_lander.py:64\u001b[0m, in \u001b[0;36mContactDetector.BeginContact\u001b[0;34m(self, contact)\u001b[0m\n\u001b[1;32m     61\u001b[0m     contactListener\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBeginContact\u001b[39m(\u001b[38;5;28mself\u001b[39m, contact):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureA\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mlander \u001b[38;5;241m==\u001b[39m contact\u001b[38;5;241m.\u001b[39mfixtureB\u001b[38;5;241m.\u001b[39mbody\n\u001b[1;32m     68\u001b[0m     ):\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mgame_over \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "observation, _ = env.reset()\n",
    "\n",
    "t = 0\n",
    "while True:\n",
    "    t += 1\n",
    "    \n",
    "    action = get_action(observation)\n",
    "    \n",
    "    new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated \n",
    "\n",
    "    timestep = Timestep(observation, action, reward, new_observation, done)\n",
    "    record_timestep(timestep)\n",
    "\n",
    "    observation = new_observation\n",
    "\n",
    "    log_reward_for_plotting(reward)\n",
    "\n",
    "\n",
    "    if done:\n",
    "        observation, _ = env.reset()\n",
    "        decrease_epsilon()\n",
    "        update_plot()\n",
    "\n",
    "    timesteps_passed = t + 1\n",
    "    \n",
    "    # Do weights update if its time to\n",
    "    if timesteps_passed % steps_until_value_iteration == 0:\n",
    "        mini_batch = sample_a_mini_batch()\n",
    "        loss = compute_loss(mini_batch)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        torch.save(main_net, main_net_path)\n",
    "\n",
    "    # Update target net if its time to\n",
    "    if timesteps_passed % steps_until_target_net_update == 0:\n",
    "        update_target_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7739a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
