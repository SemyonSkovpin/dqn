{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c76fd1-324d-4ab6-b819-c58045323902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76818d8-e24d-470f-ac67-b56e79d9239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "env_name = 'LunarLander-v3'\n",
    "mini_batch_size = 128\n",
    "steps_until_value_iteration = 500\n",
    "steps_until_target_net_update = 2000\n",
    "gamma = 0.99\n",
    "epsilon = 0.3\n",
    "lr=1e-2\n",
    "buffer_size_limit = 10000\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8e6f7-1416-4c88-9659-268fb62bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and seeds\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.reset(seed=seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e353e0-26fd-40c0-89b2-6ad6099f390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main q-network and target q-network\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "def make_mlp():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(observation_size, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, action_size)\n",
    "    )\n",
    "\n",
    "main_net = make_mlp()\n",
    "target_net = make_mlp()\n",
    "\n",
    "def update_target_net():\n",
    "    target_net.load_state_dict(main_net.state_dict())\n",
    "\n",
    "update_target_net() # They are same from the start\n",
    "\n",
    "optimiser = optim.Adam(main_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60f4b7-d6ca-4f3d-acd9-2f7c45452351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement expsilon-greedy policy\n",
    "def get_action(observation):\n",
    "    '''\n",
    "    observation: numpy array returned by env.step()\n",
    "    returns: integer action\n",
    "    '''\n",
    "    possible_actions = [i for i in range(env.action_space.n)]\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(possible_actions)\n",
    "\n",
    "    observation = torch.as_tensor(observation, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        q_star_per_each_action = main_net(observation)\n",
    "        action = torch.argmax(q_star_per_each_action).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49276b42-32af-4e4f-84a7-afd4d55455e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = deque([], maxlen=buffer_size_limit)\n",
    "\n",
    "Timestep   = namedtuple('timestep',   [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]) \n",
    "'''\n",
    "Data types of Timestep:\n",
    "state/next_state is numpy array; \n",
    "action is int; \n",
    "reward is float; \n",
    "done is bool;\n",
    "'''\n",
    "\n",
    "Mini_batch = namedtuple('mini_batch', [\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"])\n",
    "\n",
    "\n",
    "def record_timestep(timestep):\n",
    "    replay_buffer.append(timestep)\n",
    "\n",
    "\n",
    "def sample_a_mini_batch():\n",
    "    '''\n",
    "    returns: named tuple with 5 1d tensors\n",
    "    '''\n",
    "    mini_batch = random.sample(replay_buffer, mini_batch_size)\n",
    "    mini_batch = list(zip(*mini_batch)) # Transpose\n",
    "\n",
    "    # Convert list of ndarrays to ndarray because Creating a tensor from a list of numpy.ndarrays is extremely slow. \n",
    "    states = np.array(mini_batch[0])\n",
    "    next_states = np.array(mini_batch[3])\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(mini_batch[1], dtype=torch.int64)\n",
    "    rewards = torch.tensor(mini_batch[2], dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(mini_batch[4], dtype = torch.bool)\n",
    "    \n",
    "    return Mini_batch(states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af1fd9-9c67-43af-8712-be28d6b20a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mini_batch to get loss\n",
    "def compute_loss(mini_batch):\n",
    "    # Compute targets \n",
    "    v_star_of_next_states = torch.max(target_net(mini_batch.next_states), dim=1)[0]\n",
    "    v_star_of_next_states = v_star_of_next_states * (~mini_batch.dones) \n",
    "    y = mini_batch.rewards + gamma * v_star_of_next_states \n",
    "\n",
    "    # Compute main_net's predictions\n",
    "    predictions = main_net(mini_batch.states).gather(1, mini_batch.actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Loss \n",
    "    loss = nn.functional.mse_loss(predictions, y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f8e8e-90dc-4c12-953f-d4a41e0e8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "observation, _ = env.reset()\n",
    "\n",
    "t = 0\n",
    "while True:\n",
    "    t += 1\n",
    "    \n",
    "    action = get_action(observation)\n",
    "    \n",
    "    new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated \n",
    "\n",
    "    timestep = Timestep(observation, action, reward, new_observation, done)\n",
    "    record_timestep(timestep)\n",
    "\n",
    "    observation = new_observation\n",
    "    episode_return += reward\n",
    "    if done:\n",
    "        observation, _ = env.reset()\n",
    "\n",
    "    timesteps_passed = t + 1\n",
    "    \n",
    "    # Do weights update if its time to\n",
    "    if timesteps_passed % steps_until_value_iteration == 0:\n",
    "        mini_batch = sample_a_mini_batch()\n",
    "        loss = compute_loss(mini_batch)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "    # Update target net if its time to\n",
    "    if timesteps_passed % steps_until_target_net_update == 0:\n",
    "        update_target_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ecfb4-d667-402e-9fcd-2eb7d9125ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
