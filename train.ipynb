{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cd5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interactive plotting \n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c76fd1-324d-4ab6-b819-c58045323902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76818d8-e24d-470f-ac67-b56e79d9239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "env_name = 'LunarLander-v3'\n",
    "seed = 1\n",
    "main_net_path = 'main_net.pth'\n",
    "\n",
    "mini_batch_size = 128\n",
    "buffer_size_limit = 10000\n",
    "steps_until_main_net_update = 10\n",
    "steps_until_target_net_update = 500\n",
    "steps_to_train = 500000\n",
    "steps_before_learning_starts = 10000\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = 0.05\n",
    "exploration_fraction = 0.5 # Fraction of total timesteps it takes from epsilon_start to epsilon_end\n",
    "lr=2.5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c8e6f7-1416-4c88-9659-268fb62bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and seeds\n",
    "env = gym.make(env_name, render_mode='rgb_array')\n",
    "\n",
    "env.reset(seed=seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e353e0-26fd-40c0-89b2-6ad6099f390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main q-network and target q-network\n",
    "observation_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "def make_mlp():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(observation_size, 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(84, action_size)\n",
    "    )\n",
    "\n",
    "if os.path.exists(main_net_path):\n",
    "    main_net = torch.load(main_net_path)\n",
    "else:\n",
    "    main_net = make_mlp()\n",
    "    torch.save(main_net, main_net_path)\n",
    "target_net = make_mlp()\n",
    "\n",
    "def update_target_net():\n",
    "    target_net.load_state_dict(main_net.state_dict())\n",
    "\n",
    "update_target_net() # They are same from the start\n",
    "\n",
    "optimiser = optim.Adam(main_net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60f4b7-d6ca-4f3d-acd9-2f7c45452351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement expsilon-greedy policy\n",
    "def get_action(observation):\n",
    "    '''\n",
    "    observation: numpy array returned by env.step()\n",
    "    returns: integer action\n",
    "    '''\n",
    "    possible_actions = [i for i in range(env.action_space.n)]\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(possible_actions)\n",
    "\n",
    "    observation = torch.as_tensor(observation, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        q_star_per_each_action = main_net(observation)\n",
    "        action = torch.argmax(q_star_per_each_action).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11feab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon decay \n",
    "def get_current_epsilon(t):\n",
    "    slope = (epsilon_end - epsilon_start) / steps_to_train * exploration_fraction\n",
    "    epsilon = max(epsilon_end, epsilon_start +  slope * t)\n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49276b42-32af-4e4f-84a7-afd4d55455e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "replay_buffer = deque([], maxlen=buffer_size_limit)\n",
    "\n",
    "Timestep   = namedtuple('timestep',   [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]) \n",
    "'''\n",
    "Data types of Timestep:\n",
    "state/next_state is numpy array; \n",
    "action is int; \n",
    "reward is float; \n",
    "done is bool;\n",
    "'''\n",
    "\n",
    "Mini_batch = namedtuple('mini_batch', [\"states\", \"actions\", \"rewards\", \"next_states\", \"dones\"])\n",
    "\n",
    "\n",
    "def record_timestep(timestep):\n",
    "    replay_buffer.append(timestep)\n",
    "\n",
    "\n",
    "def sample_a_mini_batch():\n",
    "    '''\n",
    "    returns: named tuple with 5 1d tensors\n",
    "    '''\n",
    "    mini_batch = random.sample(replay_buffer, mini_batch_size)\n",
    "    mini_batch = list(zip(*mini_batch)) # Transpose\n",
    "\n",
    "    # Convert list of ndarrays to ndarray because Creating a tensor from a list of numpy.ndarrays is extremely slow. \n",
    "    states = np.array(mini_batch[0])\n",
    "    next_states = np.array(mini_batch[3])\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(mini_batch[1], dtype=torch.int64)\n",
    "    rewards = torch.tensor(mini_batch[2], dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    dones = torch.tensor(mini_batch[4], dtype = torch.bool)\n",
    "    \n",
    "    return Mini_batch(states, actions, rewards, next_states, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94af1fd9-9c67-43af-8712-be28d6b20a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use mini_batch to get loss\n",
    "def compute_loss(mini_batch):\n",
    "    # Compute targets \n",
    "    v_star_of_next_states = torch.max(target_net(mini_batch.next_states), dim=1)[0]\n",
    "    v_star_of_next_states = v_star_of_next_states * (~mini_batch.dones) \n",
    "    y = mini_batch.rewards + gamma * v_star_of_next_states \n",
    "\n",
    "    # Compute main_net's predictions\n",
    "    predictions = main_net(mini_batch.states).gather(1, mini_batch.actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    # Loss \n",
    "    loss = nn.functional.mse_loss(predictions, y)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fe3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot episode returns throughout training\n",
    "episode_returns = []\n",
    "smoothed_returns = []\n",
    "smooth_alpha = 0.01  # Lower = smoother\n",
    "current_episode_rewards = []\n",
    "\n",
    "# Create a separate window for the plot at the start\n",
    "plt.ion()\n",
    "fig, ax = plt.subplots()\n",
    "fig.canvas.manager.set_window_title('DQN Training Progress')\n",
    "returns_line, = ax.plot([], [], label='Episode Return', alpha=0.5)\n",
    "smooth_line, = ax.plot([], [], label='Smoothed Return', color='orange')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Return')\n",
    "ax.legend()\n",
    "fig.show()\n",
    "\n",
    "def log_reward_for_plotting(reward):\n",
    "    global current_episode_rewards\n",
    "    current_episode_rewards.append(reward)\n",
    "\n",
    "def update_plot():\n",
    "    global episode_returns, smoothed_returns, current_episode_rewards\n",
    "    episode_return = sum(current_episode_rewards)\n",
    "    episode_returns.append(episode_return)\n",
    "    # Exponential moving average for smoothing\n",
    "    if smoothed_returns:\n",
    "        new_smooth = smooth_alpha * episode_return + (1 - smooth_alpha) * smoothed_returns[-1]\n",
    "    else:\n",
    "        new_smooth = episode_return\n",
    "    smoothed_returns.append(new_smooth)\n",
    "    current_episode_rewards = []\n",
    "\n",
    "    returns_line.set_data(range(len(episode_returns)), episode_returns)\n",
    "    smooth_line.set_data(range(len(smoothed_returns)), smoothed_returns)\n",
    "    ax.relim()\n",
    "    ax.autoscale_view()\n",
    "    fig.canvas.draw()\n",
    "    fig.canvas.flush_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f8e8e-90dc-4c12-953f-d4a41e0e8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "observation, _ = env.reset()\n",
    "\n",
    "t = -1\n",
    "while True:\n",
    "    t += 1\n",
    "    timesteps_passed = t + 1\n",
    "    epsilon = get_current_epsilon(t)\n",
    "    \n",
    "    action = get_action(observation)\n",
    "    \n",
    "    new_observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated \n",
    "\n",
    "    timestep = Timestep(observation, action, reward, new_observation, done)\n",
    "    record_timestep(timestep)\n",
    "\n",
    "    observation = new_observation\n",
    "\n",
    "    log_reward_for_plotting(reward)\n",
    "\n",
    "\n",
    "    if done:\n",
    "        observation, _ = env.reset()\n",
    "        update_plot()\n",
    "\n",
    "    training_started = timesteps_passed >= steps_before_learning_starts\n",
    "    time_to_update_main = training_started and timesteps_passed % steps_until_main_net_update == 0\n",
    "    time_to_update_target = training_started and timesteps_passed % steps_until_target_net_update == 0\n",
    "    \n",
    "    # Do weights update if its time to\n",
    "    if time_to_update_main:\n",
    "        mini_batch = sample_a_mini_batch()\n",
    "        loss = compute_loss(mini_batch)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        torch.save(main_net, main_net_path)\n",
    "\n",
    "    # Update target net if its time to\n",
    "    if time_to_update_target:\n",
    "        update_target_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d51c8-843e-4443-8f8e-6963623a69b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
